{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# C Interpreter\n",
    "\n",
    "# Fourth Stage\n",
    "\n",
    "Design a grammar using an upward translator that accepts the following inputs from the C language:\n",
    "- scanf\n",
    "- main function of C language\n",
    "- void functions of C language\n",
    "\n",
    "To develop this we will divide de problem in 3 parts. First we will develop the lexical and grammar analyzer of *scanf* *main* function statement, then the *void* function and to finish we will introduce the semantic rules and the way to read the program from a file in python.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Grammar Definition for scanf\n",
    "\n",
    "The basic structure of *scanf* is:\n",
    "\n",
    "```c\n",
    " scanf(\"format\", &variable_name);\n",
    "````\n",
    "\n",
    "And we can use with multiple values:\n",
    "\n",
    "```c\n",
    " scanf(\"format\", &variable_name, &variable_name, &variable_name);\n",
    "````\n",
    "\n",
    "\n",
    "We can represent this with the following grammar:\n",
    "- scanexpr <- `SCANF` `(` stringexpr scanftail\n",
    "- scanftail <- `,` `&`fact scanftail\n",
    "- scanftrail <- `)`  \n",
    "- stringexpr <- CSTRING\n",
    "- fact -> `-` fact\n",
    "- fact -> num\n",
    "- fact -> `ID`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Implementing the Lexical Analyzer (Lexer)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sly import Lexer\n",
    "\n",
    "class Scanner(Lexer):\n",
    "    tokens = {SCANF, CSTRING, ID}\n",
    "    literals = {'(', ')', ',','&', ';'}\n",
    "\n",
    "    # Ignore whitespaces and tabs\n",
    "\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    # Regular expressions for tokens\n",
    "    CSTRING = r'\\\"(\\\\.|[^\\\"])*\\\"'\n",
    "    ID = r'[a-zA-Z][\\w_]*'\n",
    "    ID['scanf'] = SCANF\n",
    "\n",
    "    @_(r'\\d+')\n",
    "    def CNUM(self, t):\n",
    "        t.value = int(t.value)\n",
    "        return t\n",
    "\n",
    "    # Error handling rule\n",
    "\n",
    "    def error(self, t):\n",
    "        print('<-'*10,\"Illegal character '{}'\".format(t.value[0]), '->'*10)\n",
    "        self.index += 1\n",
    "        t.type='Illegal'\n",
    "        t.value =t.value[0]\n",
    "        return t"
   ]
  },
  {
   "source": [
    "## Testing lexical Analyzer(Scanf statements)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     scanf_sentences\n",
       "0  scanf(\"%d %d %d %d\", &minx, &maxx, &miny, &maxy);\n",
       "1  scanf(\"%s %d %f %c\", &var1, &var2, &var3, &var4);\n",
       "2                                 scanf(\"%s\",&name);"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scanf_sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>scanf(\"%d %d %d %d\", &amp;minx, &amp;maxx, &amp;miny, &amp;maxy);</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>scanf(\"%s %d %f %c\", &amp;var1, &amp;var2, &amp;var3, &amp;var4);</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>scanf(\"%s\",&amp;name);</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('../assets/testing/scanf_sentences.csv', delimiter=\"'\")\n",
    "data[['scanf_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------------------------------------\n0 Lexically Testing sentence: 'scanf(\"%d %d %d %d\", &minx, &maxx, &miny, &maxy);'\n--------------------------------------------------------------------------------\n type = 'SCANF', value = 'scanf'\n type = '(', value = '('\n type = 'CSTRING', value = '\"%d %d %d %d\"'\n type = ',', value = ','\n type = '&', value = '&'\n type = 'ID', value = 'minx'\n type = ',', value = ','\n type = '&', value = '&'\n type = 'ID', value = 'maxx'\n type = ',', value = ','\n type = '&', value = '&'\n type = 'ID', value = 'miny'\n type = ',', value = ','\n type = '&', value = '&'\n type = 'ID', value = 'maxy'\n type = ')', value = ')'\n type = ';', value = ';'\n--------------------------------------------------------------------------------\n1 Lexically Testing sentence: 'scanf(\"%s %d %f %c\", &var1, &var2, &var3, &var4);'\n--------------------------------------------------------------------------------\n type = 'SCANF', value = 'scanf'\n type = '(', value = '('\n type = 'CSTRING', value = '\"%s %d %f %c\"'\n type = ',', value = ','\n type = '&', value = '&'\n type = 'ID', value = 'var1'\n type = ',', value = ','\n type = '&', value = '&'\n type = 'ID', value = 'var2'\n type = ',', value = ','\n type = '&', value = '&'\n type = 'ID', value = 'var3'\n type = ',', value = ','\n type = '&', value = '&'\n type = 'ID', value = 'var4'\n type = ')', value = ')'\n type = ';', value = ';'\n--------------------------------------------------------------------------------\n2 Lexically Testing sentence: 'scanf(\"%s\",&name);'\n--------------------------------------------------------------------------------\n type = 'SCANF', value = 'scanf'\n type = '(', value = '('\n type = 'CSTRING', value = '\"%s\"'\n type = ',', value = ','\n type = '&', value = '&'\n type = 'ID', value = 'name'\n type = ')', value = ')'\n type = ';', value = ';'\n"
     ]
    }
   ],
   "source": [
    "lexer = Scanner()\n",
    "sentences = data['scanf_sentences'].values\n",
    "pass_or_not = []\n",
    "all_token_pass = True\n",
    "\n",
    "for index, sentence in enumerate(sentences):\n",
    "    print('-' * 80,\"{} Lexically Testing sentence: '{}'\".format(index, sentence),'-' * 80, sep='\\n')\n",
    "    for token in lexer.tokenize(sentence):\n",
    "        print(\" type = '{}', value = '{}'\".format(token.type, token.value))\n",
    "        if all_token_pass and 'Illegal' in token.type:\n",
    "            all_token_pass = False\n",
    "    \n",
    "    pass_or_not.append('Pass') if all_token_pass else pass_or_not.append('FAIL')\n",
    "    all_token_pass = True\n",
    "\n",
    "data['Test'] = pass_or_not"
   ]
  },
  {
   "source": [
    "## 2. Grammar Definition for void functions\n",
    "\n",
    "The basic structure of *void function* is:\n",
    "\n",
    "```c\n",
    "void fuction_name([parameters]*) { instructions }\n",
    "````\n",
    "We can represent this with the following grammar:\n",
    "\n",
    "- functionDec <- `VOID` ID`(` parameters `)` `{`instructions`}`\n",
    "- instructions <- def instructions\n",
    "- instructions <- statements instructions\n",
    "- instructions <- `e`\n",
    "- parameters <- ID parameters'\n",
    "- parameters' <-, ID parameters'\n",
    "- parameters' <- `e`\n",
    "- statements <- scanexpr\n",
    "- scanexpr <- `SCANF` `(` stringexpr scanftail\n",
    "- scanftail <- `,` `&`fact scanftail\n",
    "- scanftrail <- `)`  \n",
    "- stringexpr <- CSTRING\n",
    "- fact -> `-` fact\n",
    "- fact -> num\n",
    "- fact -> `ID`\n",
    "\n",
    "The basic structure for call functions is:\n",
    "\n",
    "- def -> asign `;`\n",
    "- asign -> ID `=` asign\n",
    "- asign -> expr\n",
    "- expr -> functionexpr\n",
    "- functionexpr <- ID`(` arguments `)`\n",
    "- arguments <- fact, arguments\n",
    "- arguments <- `e` \n",
    "- fact -> `-` fact\n",
    "- fact -> num\n",
    "- fact -> `ID`\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.Testing lexical analizer (Void functions)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                      void_functions\n",
       "0  void test(){a=0; b=0; scanf(\"%d %d\", a,b); c =...\n",
       "1           void test(a , b){ c = b + 3; a = a + 5;}\n",
       "2                                            test();\n",
       "3                                        test(5, 2);\n",
       "4                                        test(a, b);"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>void_functions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>void test(){a=0; b=0; scanf(\"%d %d\", a,b); c =...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>void test(a , b){ c = b + 3; a = a + 5;}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test();</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test(5, 2);</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test(a, b);</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('../assets/testing/void_functions_sentences.csv', delimiter=\"'\")\n",
    "data[['void_functions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Scanner(Lexer):\n",
    "    tokens = {ID, CNUM, EQ, NEQ, OR, AND, LEQ, GEQ, IF, VOID, SCANF, CSTRING}\n",
    "    literals ={'=', '<', '>', '/', '!', '+', '-' , '*', ';', '{', '}', '(', ')', ',', '&'}\n",
    "\n",
    "    # Ignore whitespace and tabulations\n",
    "\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    # Regular expressions rules for tokens\n",
    "\n",
    "    ID = r'[a-zA-Z][\\w_]*'\n",
    "    EQ = r'=='\n",
    "    NEQ = r'!='\n",
    "    GEQ = r'>='\n",
    "    LEQ = r'<='\n",
    "    AND = r'&&'\n",
    "    OR = r'\\|\\|'\n",
    "    \n",
    "    CSTRING = r'\\\"(\\\\.|[^\\\"])*\\\"'\n",
    "\n",
    "    # Special cases\n",
    "    ID['if'] = IF\n",
    "    ID['void'] = VOID\n",
    "    ID['scanf'] = SCANF\n",
    "\n",
    "\n",
    "    @_(r'\\d+')\n",
    "    def CNUM(self, t):\n",
    "        t.value = int(t.value)\n",
    "        return t\n",
    "\n",
    "    # Error handling rule\n",
    "\n",
    "    def error(self, t):\n",
    "        print('<-'*10,\"Illegal character '{}'\".format(t.value[0]), '->'*10)\n",
    "        self.index += 1\n",
    "        t.type='Illegal'\n",
    "        t.value =t.value[0]\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------------------------------------\n0 Lexically Testing sentence: 'void test(){a=0; b=0; scanf(\"%d %d\", a,b); c = 3 + 2 + a; d = 5 + b;}'\n--------------------------------------------------------------------------------\n type = 'VOID', value = 'void'\n type = 'ID', value = 'test'\n type = '(', value = '('\n type = ')', value = ')'\n type = '{', value = '{'\n type = 'ID', value = 'a'\n type = '=', value = '='\n type = 'CNUM', value = '0'\n type = ';', value = ';'\n type = 'ID', value = 'b'\n type = '=', value = '='\n type = 'CNUM', value = '0'\n type = ';', value = ';'\n type = 'SCANF', value = 'scanf'\n type = '(', value = '('\n type = 'CSTRING', value = '\"%d %d\"'\n type = ',', value = ','\n type = 'ID', value = 'a'\n type = ',', value = ','\n type = 'ID', value = 'b'\n type = ')', value = ')'\n type = ';', value = ';'\n type = 'ID', value = 'c'\n type = '=', value = '='\n type = 'CNUM', value = '3'\n type = '+', value = '+'\n type = 'CNUM', value = '2'\n type = '+', value = '+'\n type = 'ID', value = 'a'\n type = ';', value = ';'\n type = 'ID', value = 'd'\n type = '=', value = '='\n type = 'CNUM', value = '5'\n type = '+', value = '+'\n type = 'ID', value = 'b'\n type = ';', value = ';'\n type = '}', value = '}'\n--------------------------------------------------------------------------------\n1 Lexically Testing sentence: 'void test(a , b){ c = b + 3; a = a + 5;}'\n--------------------------------------------------------------------------------\n type = 'VOID', value = 'void'\n type = 'ID', value = 'test'\n type = '(', value = '('\n type = 'ID', value = 'a'\n type = ',', value = ','\n type = 'ID', value = 'b'\n type = ')', value = ')'\n type = '{', value = '{'\n type = 'ID', value = 'c'\n type = '=', value = '='\n type = 'ID', value = 'b'\n type = '+', value = '+'\n type = 'CNUM', value = '3'\n type = ';', value = ';'\n type = 'ID', value = 'a'\n type = '=', value = '='\n type = 'ID', value = 'a'\n type = '+', value = '+'\n type = 'CNUM', value = '5'\n type = ';', value = ';'\n type = '}', value = '}'\n--------------------------------------------------------------------------------\n2 Lexically Testing sentence: 'test();'\n--------------------------------------------------------------------------------\n type = 'ID', value = 'test'\n type = '(', value = '('\n type = ')', value = ')'\n type = ';', value = ';'\n--------------------------------------------------------------------------------\n3 Lexically Testing sentence: 'test(5, 2);'\n--------------------------------------------------------------------------------\n type = 'ID', value = 'test'\n type = '(', value = '('\n type = 'CNUM', value = '5'\n type = ',', value = ','\n type = 'CNUM', value = '2'\n type = ')', value = ')'\n type = ';', value = ';'\n--------------------------------------------------------------------------------\n4 Lexically Testing sentence: 'test(a, b);'\n--------------------------------------------------------------------------------\n type = 'ID', value = 'test'\n type = '(', value = '('\n type = 'ID', value = 'a'\n type = ',', value = ','\n type = 'ID', value = 'b'\n type = ')', value = ')'\n type = ';', value = ';'\n"
     ]
    }
   ],
   "source": [
    "lexer = Scanner()\n",
    "sentences = data['void_functions'].values\n",
    "pass_or_not = []\n",
    "all_token_pass = True\n",
    "\n",
    "for index, sentence in enumerate(sentences):\n",
    "    print('-' * 80,\"{} Lexically Testing sentence: '{}'\".format(index, sentence),'-' * 80, sep='\\n')\n",
    "    for token in lexer.tokenize(sentence):\n",
    "        print(\" type = '{}', value = '{}'\".format(token.type, token.value))\n",
    "        if all_token_pass and 'Illegal' in token.type:\n",
    "            all_token_pass = False\n",
    "    \n",
    "    pass_or_not.append('Pass') if all_token_pass else pass_or_not.append('FAIL')\n",
    "    all_token_pass = True\n",
    "\n",
    "data['Test'] = pass_or_not"
   ]
  }
 ]
}