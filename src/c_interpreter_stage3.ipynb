{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# C Interpreter\n",
    "\n",
    "# Third Stage\n",
    "\n",
    "Design a grammar using an a upward translator that accepts the following inputs from the C language:\n",
    "- `printf` functions from C language.\n",
    "\n",
    "Add the AST for arithmetic, logic and relation operators.\n",
    "\n",
    "To develop this we will desing the grammar that accepts `printf` statements and then implement the AST for arithmetic, logic and relation operators."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Grammar design: Printf statement\n",
    "\n",
    "We will start developing the *printf* statement:\n",
    "\n",
    "### C print statement (printf)\n",
    "\n",
    "In *C* the base structure of the print statement is:\n",
    "\n",
    "``` \n",
    "printf(\"string [%type]*\", [variable_name])\n",
    "```\n",
    "\n",
    "Could contains variables or not, and the numbers of tags must match the number of variables.\n",
    "\n",
    "- printexpr <- `PRINTF` `(` stringexpr printftail\n",
    "- printftrail <- `,` fact printftail\n",
    "- printftrail <- `)`  \n",
    "- stringexpr <- CSTRING\n",
    "- fact -> `-` fact\n",
    "- fact -> num\n",
    "- fact -> `ID`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Implementing the lexical analyzer (Lexer)\n",
    "\n",
    "We will reuse the main lexical analyzer implementation, adding the *printf* logic necessary to process the new rules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sly import Lexer\n",
    "\n",
    "class Scanner(Lexer):\n",
    "    tokens = {ID, CNUM, PRINTF, STYPE, CSTRING}\n",
    "    literals ={'(', ')',',',';'}\n",
    "\n",
    "    # Ignore whitespace and tabulations\n",
    "\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    # Regular expressions rules for tokens\n",
    "\n",
    "    ID = r'[a-zA-Z][\\w_]*'\n",
    "    CSTRING = r'\\\"(\\\\.|[^\\\"])*\\\"'\n",
    "\n",
    "    # Special cases\n",
    "    ID['printf'] = PRINTF\n",
    "\n",
    "    @_(r'\\d+')\n",
    "    def CNUM(self, t):\n",
    "        t.value = int(t.value)\n",
    "        return t\n",
    "\n",
    "    # Error handling rule\n",
    "\n",
    "    def error(self, t):\n",
    "        print('<-'*10,\"Illegal character '{}'\".format(t.value[0]), '->'*10)\n",
    "        self.index += 1\n",
    "        t.type='Illegal'\n",
    "        t.value =t.value[0]\n",
    "        return t"
   ]
  },
  {
   "source": [
    "## 1. Testing lexical Analyzer (printf)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                printf_sentences\n",
       "0             printf(\"%d %d %d %d\", a, b, c, d);\n",
       "1                        printf(\"%d %f\", 8, 45);\n",
       "2  printf(\"the add is %d the sub is %f\", 8, 45);"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>printf_sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>printf(\"%d %d %d %d\", a, b, c, d);</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>printf(\"%d %f\", 8, 45);</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>printf(\"the add is %d the sub is %f\", 8, 45);</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('../assets/testing/printf_sentences.csv', delimiter=\"'\")\n",
    "data[['printf_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Scanner' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c3993f3efe01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScanner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'printf_sentences'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpass_or_not\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mall_token_pass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Scanner' is not defined"
     ]
    }
   ],
   "source": [
    "lexer = Scanner()\n",
    "sentences = data['printf_sentences'].values\n",
    "pass_or_not = []\n",
    "all_token_pass = True\n",
    "\n",
    "for index, sentence in enumerate(sentences):\n",
    "    print('-' * 80,\"{} Lexically Testing sentence: '{}'\".format(index, sentence),'-' * 80, sep='\\n')\n",
    "    for token in lexer.tokenize(sentence):\n",
    "        print(\" type = '{}', value = '{}'\".format(token.type, token.value))\n",
    "        if all_token_pass and 'Illegal' in token.type:\n",
    "            all_token_pass = False\n",
    "    \n",
    "    pass_or_not.append('Pass') if all_token_pass else pass_or_not.append('FAIL')\n",
    "    all_token_pass = True\n",
    "\n",
    "data['Test'] = pass_or_not"
   ]
  },
  {
   "source": [
    "## 2. Developing the AST\n",
    "\n",
    "Per symbol with a semtantic associated (operations, control statements, ID...) we have to implement a Node. But we can refactor operations using  Abstract class, and generalizing by operator type (unary or binary)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def write(self):\n",
    "        pass\n",
    "\n",
    "class BinaryOpNode(Node):\n",
    "    def __init__(self, op, p1, p2):\n",
    "        self.operation = op\n",
    "        self.pn1 = p1\n",
    "        self.pn2 = p2\n",
    "    \n",
    "    def write(self):\n",
    "        print(\"{} {} {} = {}\".format(self.pn1, self.operation, self.pn2, eval(str(self.pn1) + self.operation + str(self.pn2))))\n",
    "\n",
    "class UnaryOpNode(Node):\n",
    "    def __init__(self, op, p1):\n",
    "        self.operation = op\n",
    "        self.pn1 = p1\n",
    "\n",
    "    def write(self):\n",
    "        print(\"{} {} = {}\".format( self.operation, self.pn1, eval(self.operation+\" \"+str(self.pn1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4 + 5 = 9\nnot True = False\n- 4 = -4\n"
     ]
    }
   ],
   "source": [
    "sum_op = BinaryOpNode('+', 4 ,5)\n",
    "sum_op.write()\n",
    "\n",
    "not_op = UnaryOpNode('not', True)\n",
    "not_op.write()\n",
    "\n",
    "neg_op = UnaryOpNode('-', 4)\n",
    "neg_op.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}