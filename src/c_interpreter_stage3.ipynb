{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# C Interpreter\n",
    "\n",
    "# Third Stage\n",
    "\n",
    "Design a grammar using an a upward translator that accepts the following inputs from the C language:\n",
    "- `printf` functions from C language.\n",
    "\n",
    "Add the AST for arithmetic, logic and relation operators.\n",
    "\n",
    "To develop this we will desing the grammar that accepts `printf` statements and then implement the AST for arithmetic, logic and relation operators."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Grammar design: Printf statement\n",
    "\n",
    "We will start developing the *printf* statement:\n",
    "\n",
    "### C print statement (printf)\n",
    "\n",
    "In *C* the base structure of the print statement is:\n",
    "\n",
    "``` \n",
    "printf(\"string [%type]*\", [variable_name])\n",
    "```\n",
    "\n",
    "Could contains variables or not, and the numbers of tags must match the number of variables.\n",
    "\n",
    "- printexpr <- `PRINTF` `(` stringexpr printftail\n",
    "- printftrail <- `,` fact printftail\n",
    "- printftrail <- `)`  \n",
    "- stringexpr <- CSTRING\n",
    "- fact -> `-` fact\n",
    "- fact -> num\n",
    "- fact -> `ID`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Implementing the lexical analyzer (Lexer)\n",
    "\n",
    "We will reuse the main lexical analyzer implementation, adding the *printf* logic necessary to process the new rules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sly import Lexer\n",
    "\n",
    "class Scanner(Lexer):\n",
    "    tokens = {ID, CNUM, PRINTF, CSTRING, IF}\n",
    "    literals ={'(', ')', ',', ';', '+', '-', '*', '/'}\n",
    "\n",
    "    # Ignore whitespace and tabulations\n",
    "\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    # Regular expressions rules for tokens\n",
    "\n",
    "    ID = r'[a-zA-Z][\\w_]*'\n",
    "    CSTRING = r'\\\"(\\\\.|[^\\\"])*\\\"'\n",
    "\n",
    "    # Special cases\n",
    "    ID['printf'] = PRINTF\n",
    "\n",
    "    @_(r'\\d+')\n",
    "    def CNUM(self, t):\n",
    "        t.value = int(t.value)\n",
    "        return t\n",
    "\n",
    "    # Error handling rule\n",
    "\n",
    "    def error(self, t):\n",
    "        print('<-'*10,\"Illegal character '{}'\".format(t.value[0]), '->'*10)\n",
    "        self.index += 1\n",
    "        t.type='Illegal'\n",
    "        t.value =t.value[0]\n",
    "        return t"
   ]
  },
  {
   "source": [
    "## 1. Testing lexical Analyzer (printf)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                printf_sentences\n",
       "0             printf(\"%d %d %d %d\", a, b, c, d);\n",
       "1                        printf(\"%d %f\", 8, 45);\n",
       "2  printf(\"the add is %d the sub is %f\", 8, 45);"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>printf_sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>printf(\"%d %d %d %d\", a, b, c, d);</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>printf(\"%d %f\", 8, 45);</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>printf(\"the add is %d the sub is %f\", 8, 45);</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('../assets/testing/printf_sentences.csv', delimiter=\"'\")\n",
    "data[['printf_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------------------------------------\n0 Lexically Testing sentence: 'printf(\"%d %d %d %d\", a, b, c, d);'\n--------------------------------------------------------------------------------\n type = 'PRINTF', value = 'printf'\n type = '(', value = '('\n type = 'CSTRING', value = '\"%d %d %d %d\"'\n type = ',', value = ','\n type = 'ID', value = 'a'\n type = ',', value = ','\n type = 'ID', value = 'b'\n type = ',', value = ','\n type = 'ID', value = 'c'\n type = ',', value = ','\n type = 'ID', value = 'd'\n type = ')', value = ')'\n type = ';', value = ';'\n--------------------------------------------------------------------------------\n1 Lexically Testing sentence: 'printf(\"%d %f\", 8, 45);'\n--------------------------------------------------------------------------------\n type = 'PRINTF', value = 'printf'\n type = '(', value = '('\n type = 'CSTRING', value = '\"%d %f\"'\n type = ',', value = ','\n type = 'CNUM', value = '8'\n type = ',', value = ','\n type = 'CNUM', value = '45'\n type = ')', value = ')'\n type = ';', value = ';'\n--------------------------------------------------------------------------------\n2 Lexically Testing sentence: 'printf(\"the add is %d the sub is %f\", 8, 45);'\n--------------------------------------------------------------------------------\n type = 'PRINTF', value = 'printf'\n type = '(', value = '('\n type = 'CSTRING', value = '\"the add is %d the sub is %f\"'\n type = ',', value = ','\n type = 'CNUM', value = '8'\n type = ',', value = ','\n type = 'CNUM', value = '45'\n type = ')', value = ')'\n type = ';', value = ';'\n"
     ]
    }
   ],
   "source": [
    "lexer = Scanner()\n",
    "sentences = data['printf_sentences'].values\n",
    "pass_or_not = []\n",
    "all_token_pass = True\n",
    "\n",
    "for index, sentence in enumerate(sentences):\n",
    "    print('-' * 80,\"{} Lexically Testing sentence: '{}'\".format(index, sentence),'-' * 80, sep='\\n')\n",
    "    for token in lexer.tokenize(sentence):\n",
    "        print(\" type = '{}', value = '{}'\".format(token.type, token.value))\n",
    "        if all_token_pass and 'Illegal' in token.type:\n",
    "            all_token_pass = False\n",
    "    \n",
    "    pass_or_not.append('Pass') if all_token_pass else pass_or_not.append('FAIL')\n",
    "    all_token_pass = True\n",
    "\n",
    "data['Test'] = pass_or_not"
   ]
  },
  {
   "source": [
    "## 2. Developing the AST\n",
    "\n",
    "Per Token and rules with a semantic associated (operations, control statements, ID...) we have to implement a Node. But we can refactor operations using  Abstract class, and generalizing by operator type (unary or binary)\n",
    "\n",
    "Now we can include this into our grammar and test it, remember that\n",
    "the condition and the instructions of the *if* statement has more priority so: \n",
    "\n",
    "- exprIF -> if `(` condition `)` instruction\n",
    "- condition -> expr\n",
    "- instruction -> `{` exprins `;` `}`\n",
    "- instruction -> def\n",
    "- exprins -> exprins `;` asing\n",
    "- exprins -> asign\n",
    "\n",
    "\n",
    "- def -> asign `;`\n",
    "- asign -> ID `=` asign\n",
    "- asign -> expr\n",
    "- expr -> exprOR\n",
    "- exprOR -> exprOR `||` exprAND\n",
    "- exprOR -> exprAND\n",
    "- exprAND -> exprAND `&&` exprE\n",
    "- exprAND -> exprE\n",
    "- exprE -> exprE `[==, !=]` exprC\n",
    "- exprE -> exprC\n",
    "- exprC -> exprC `[<, <=, >, >=]` exprA\n",
    "- exprC -> exprA\n",
    "- exprA -> exprA `[+, -] ` add\n",
    "- exprA -> add\n",
    "- add -> add `[*, /]` fact\n",
    "- add -> fact\n",
    "- fact -> `-` fact\n",
    "- fact -> num\n",
    "- fact -> `ID`\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {}\n",
    "\n",
    "class Node():\n",
    "    def write(self):\n",
    "        pass\n",
    "\n",
    "class BinaryOpNode(Node):\n",
    "    def __init__(self, op, p1, p2):\n",
    "        self.operation = op\n",
    "        self.pn1 = p1\n",
    "        self.pn2 = p2\n",
    "    \n",
    "    def write(self):\n",
    "        print(\"{} {} {} = {}\".format(self.pn1, self.operation, self.pn2, eval(str(self.pn1) + self.operation + str(self.pn2))))\n",
    "\n",
    "class UnaryOpNode(Node):\n",
    "    def __init__(self, op, p1):\n",
    "        self.operation = op\n",
    "        self.pn1 = p1\n",
    "\n",
    "    def write(self):\n",
    "        print(\"{} {} = {}\".format( self.operation, self.pn1, eval(self.operation+\" \"+str(self.pn1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4 + 5 = 9\nnot True = False\n- 4 = -4\n"
     ]
    }
   ],
   "source": [
    "sum_op = BinaryOpNode('+', 4 ,5)\n",
    "sum_op.write()\n",
    "\n",
    "not_op = UnaryOpNode('not', True)\n",
    "not_op.write()\n",
    "\n",
    "neg_op = UnaryOpNode('-', 4)\n",
    "neg_op.write()"
   ]
  },
  {
   "source": [
    "Now the tokens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "class NodeId(Node):\n",
    "    def __init__(self, n):\n",
    "        self.name = n\n",
    "        self.value = table[n]\n",
    "    def write(self):\n",
    "        print('ID [{}] = {}'.format(self.name, self.value))\n",
    "\n",
    "class NodeNum(Node):\n",
    "    def __init__(self, v):\n",
    "        self.value = v\n",
    "    def write(self):\n",
    "        print(\"Num value = {}\".format(self.value))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID [a] = 4\nNum value = 5\n"
     ]
    }
   ],
   "source": [
    "table['a'] = 4\n",
    "\n",
    "id = NodeId('a')\n",
    "id.write()\n",
    "\n",
    "num = NodeNum(5)\n",
    "num.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: Token(s) {PRINTF,CNUM,CSTRING} defined, but not used\nWARNING: There are 3 unused tokens\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "YaccError",
     "evalue": "Unable to build grammar.\nInfinite recursion detected for symbol 'asign'\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mYaccError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f46c7003cf16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msly\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mCInterpreterParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScanner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\los_g\\OneDrive\\Documentos\\PL\\PL-2020\\prácticas\\c-interpreter\\.env\\lib\\site-packages\\sly\\yacc.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(meta, clsname, bases, attributes)\u001b[0m\n\u001b[0;32m   1772\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1773\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1774\u001b[1;33m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1775\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\los_g\\OneDrive\\Documentos\\PL\\PL-2020\\prácticas\\c-interpreter\\.env\\lib\\site-packages\\sly\\yacc.py\u001b[0m in \u001b[0;36m_build\u001b[1;34m(cls, definitions)\u001b[0m\n\u001b[0;32m   1966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1967\u001b[0m         \u001b[1;31m# Build the underlying grammar object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1968\u001b[1;33m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__build_grammar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m         \u001b[1;31m# Build the LR tables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\los_g\\OneDrive\\Documentos\\PL\\PL-2020\\prácticas\\c-interpreter\\.env\\lib\\site-packages\\sly\\yacc.py\u001b[0m in \u001b[0;36m__build_grammar\u001b[1;34m(cls, rules)\u001b[0m\n\u001b[0;32m   1912\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1914\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mYaccError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unable to build grammar.\\n'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mYaccError\u001b[0m: Unable to build grammar.\nInfinite recursion detected for symbol 'asign'\n"
     ]
    }
   ],
   "source": [
    "from sly import Parser\n",
    "\n",
    "class CInterpreterParser(Parser):\n",
    "    tokens = Scanner.tokens\n",
    "\n",
    "    def __init__(self):\n",
    "        self.table = {}\n",
    "\n",
    "    @_('ID \"=\" asign')\n",
    "    def asign(self, p):\n",
    "        return BinaryNode(\"=\", NodeId(p.ID), str(p.asign))"
   ]
  }
 ]
}